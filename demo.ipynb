{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531467a2-5160-4073-a990-0d81d574b014",
   "metadata": {},
   "source": [
    "## (1) Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff514ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9337043-4e7a-4b20-9d89-6c6257245334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of:\n",
    "#     'state-spaces/mamba-2.8b-slimpj'\n",
    "#     'state-spaces/mamba-2.8b'\n",
    "#     'state-spaces/mamba-1.4b'\n",
    "#     'state-spaces/mamba-790m'\n",
    "#     'state-spaces/mamba-370m'\n",
    "#     'state-spaces/mamba-130m'\n",
    "pretrained_model_name = 'state-spaces/mamba-130m'\n",
    "dummy_input = \"test\"\n",
    "\n",
    "model = Mamba.from_pretrained(pretrained_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "input_ids = tokenizer(dummy_input, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export model\n",
    "export_name = \"mamba_model\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    input_ids,  \n",
    "    f\"{export_name}.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'seq_length'},  \n",
    "        'output': {0: 'batch_size', 1: 'seq_length'}\n",
    "    }\n",
    ")\n",
    "torch.save(model, f\"{export_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddaf2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export minimal model\n",
    "\n",
    "args = ModelArgs(\n",
    "    d_model=5,\n",
    "    n_layer=1,\n",
    "    vocab_size=50277\n",
    ")\n",
    "model_1 = Mamba(args)\n",
    "model_1.eval()\n",
    "export_name = \"mamba_minimal_1_layer\"\n",
    "\n",
    "torch.save(model_1, f\"{export_name}.pt\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_1,\n",
    "    input_ids,  \n",
    "    f\"{export_name}.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'seq_length'},  \n",
    "        'output': {0: 'batch_size', 1: 'seq_length'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession('mamba_minimal_1_layer.onnx')\n",
    "\n",
    "# Generate a model dummy input\n",
    "dummy_prompt_1 = \"Harry test ciao bla bla\"  \n",
    "tokens_1 = tokenizer(dummy_prompt_1, return_tensors=\"pt\")\n",
    "input_ids_1 = tokens_1.input_ids.to(device=\"cpu\")\n",
    "input_ids_np = np.array(input_ids_1)\n",
    "print(input_ids_np.shape)\n",
    "\n",
    "# Inference\n",
    "inputs = {ort_session.get_inputs()[0].name: input_ids_np}\n",
    "out = ort_session.run(None, inputs)\n",
    "\n",
    "# Output\n",
    "print(input_ids_np.shape)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0825a",
   "metadata": {},
   "source": [
    "## Export to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c40fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nobuco\n",
    "from nobuco import ChannelOrder, ChannelOrderingStrategy\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412eb28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_link, source_code = nobuco.locate_converter(F.softplus)\n",
    "print('Converter location:')\n",
    "print(location_link)\n",
    "print('Converter source code:')\n",
    "print(source_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f353091",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nobuco.converter(F.softplus, channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS)\n",
    "def softplus(input: torch.Tensor):\n",
    "    print(input)\n",
    "    return lambda input: tf.keras.activations.softplus(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61811201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8311, -1.0473,  0.0900,  0.1249,  0.7722, -0.5626, -0.0159,\n",
      "          -0.8217, -0.1600,  0.6595]]])\n",
      "Legend:\n",
      "    \u001b[32mGreen\u001b[0m — conversion successful\n",
      "    \u001b[33mYellow\u001b[0m — conversion imprecise\n",
      "    \u001b[31mRed\u001b[0m — conversion failed\n",
      "    \u001b[31m\u001b[7mRed\u001b[0m — no converter found\n",
      "    \u001b[0m\u001b[1mBold\u001b[0m — conversion applied directly\n",
      "    * — subgraph reused\n",
      "    \u001b[7mTensor\u001b[0m — this output is not dependent on any of subgraph's input tensors\n",
      "    \u001b[4mTensor\u001b[0m — this input is a parameter / constant\n",
      "    \u001b[90mTensor\u001b[0m — this tensor is useless\n",
      "\n",
      "\u001b[32mMamba[model]\u001b[0m(int64_0<1,1>\u001b[0m) -> float32_87<1,1,50280>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32mEmbedding[torch.nn.modules.sparse]\u001b[0m(int64_0<1,1>\u001b[0m) -> float32_2<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m └·\u001b[0m \u001b[32m\u001b[1membedding[torch.nn.functional]\u001b[0m(int64_0<1,1>\u001b[0m, \u001b[4mfloat32_1<50280,5>\u001b[0m, None, None, 2.0, False, False) -> float32_2<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32mResidualBlock[model]\u001b[0m(float32_2<1,1,5>\u001b[0m) -> float32_79<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32mRMSNorm[model]\u001b[0m(float32_2<1,1,5>\u001b[0m) -> float32_9<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mpow[torch.Tensor]\u001b[0m(float32_2<1,1,5>\u001b[0m, 2) -> float32_3<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mmean[torch.Tensor]\u001b[0m(float32_3<1,1,5>\u001b[0m, -1, keepdim=True) -> float32_4<1,1,1>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__add__[torch.Tensor]\u001b[0m(float32_4<1,1,1>\u001b[0m, 1e-05) -> float32_5<1,1,1>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mrsqrt[torch]\u001b[0m(float32_5<1,1,1>\u001b[0m) -> float32_6<1,1,1>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__mul__[torch.Tensor]\u001b[0m(float32_2<1,1,5>\u001b[0m, float32_6<1,1,1>\u001b[0m) -> float32_7<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m └·\u001b[0m \u001b[32m\u001b[1m__mul__[torch.Tensor]\u001b[0m(float32_7<1,1,5>\u001b[0m, \u001b[4mfloat32_8<5>\u001b[0m) -> float32_9<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32mMambaBlock[model]\u001b[0m(float32_9<1,1,5>\u001b[0m) -> float32_78<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mshape[nobuco]\u001b[0m(float32_9<1,1,5>\u001b[0m) -> (\u001b[90mint32_10<>\u001b[0m, int32_11<>\u001b[0m, \u001b[90mint32_12<>\u001b[0m)\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mLinear[torch.nn.modules.linear]\u001b[0m(float32_9<1,1,5>\u001b[0m) -> float32_14<1,1,20>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m └·\u001b[0m \u001b[0mlinear[torch.nn.functional]\u001b[0m(float32_9<1,1,5>\u001b[0m, float32_13<20,5>\u001b[0m, None) -> float32_14<1,1,20>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1msplit[torch.Tensor]\u001b[0m(float32_14<1,1,20>\u001b[0m, split_size=[10, 10], dim=-1) -> (float32_15<1,1,10>\u001b[0m, float32_16<1,1,10>\u001b[0m)\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mshape[nobuco]\u001b[0m(float32_15<1,1,10>\u001b[0m) -> (\u001b[90mint32_17<>\u001b[0m, \u001b[90mint32_18<>\u001b[0m, \u001b[90mint32_19<>\u001b[0m)\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mshape[nobuco]\u001b[0m(float32_15<1,1,10>\u001b[0m) -> (int32_20<>\u001b[0m, int32_21<>\u001b[0m, int32_22<>\u001b[0m)\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__rmul__[torch.Tensor]\u001b[0m(int32_20<>\u001b[0m, 1) -> \u001b[90mint32_23<>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__rmul__[torch.Tensor]\u001b[0m(int32_22<>\u001b[0m, 1) -> \u001b[90mint32_24<>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__rmul__[torch.Tensor]\u001b[0m(int32_21<>\u001b[0m, 1) -> \u001b[90mint32_25<>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mpermute[torch.Tensor]\u001b[0m(float32_15<1,1,10>\u001b[0m, [0, 2, 1]) -> float32_26<1,10,1>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mConv1d[torch.nn.modules.conv]\u001b[0m(float32_26<1,10,1>\u001b[0m) -> float32_29<1,10,4>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m └·\u001b[0m \u001b[0mconv1d[torch.nn.functional]\u001b[0m(float32_26<1,10,1>\u001b[0m, float32_27<10,1,4>\u001b[0m, float32_28<10>\u001b[0m, (1), (3), (1), 10) -> float32_29<1,10,4>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__getitem__[TensorBase]\u001b[0m(float32_29<1,10,4>\u001b[0m, (:, :, :int32_11<>\u001b[0m)) -> float32_30<1,10,1>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mshape[nobuco]\u001b[0m(float32_30<1,10,1>\u001b[0m) -> (\u001b[90mint32_31<>\u001b[0m, \u001b[90mint32_32<>\u001b[0m, \u001b[90mint32_33<>\u001b[0m)\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mshape[nobuco]\u001b[0m(float32_30<1,10,1>\u001b[0m) -> (int32_34<>\u001b[0m, int32_35<>\u001b[0m, int32_36<>\u001b[0m)\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__rmul__[torch.Tensor]\u001b[0m(int32_34<>\u001b[0m, 1) -> \u001b[90mint32_37<>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__rmul__[torch.Tensor]\u001b[0m(int32_36<>\u001b[0m, 1) -> \u001b[90mint32_38<>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__rmul__[torch.Tensor]\u001b[0m(int32_35<>\u001b[0m, 1) -> \u001b[90mint32_39<>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mpermute[torch.Tensor]\u001b[0m(float32_30<1,10,1>\u001b[0m, [0, 2, 1]) -> float32_40<1,1,10>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1msilu[torch.nn.functional]\u001b[0m(float32_40<1,1,10>\u001b[0m) -> float32_41<1,1,10>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mshape[nobuco]\u001b[0m(\u001b[4mfloat32_42<10,16>\u001b[0m) -> (int32_43<>\u001b[0m, int32_44<>\u001b[0m)\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mfloat[torch.Tensor]\u001b[0m(\u001b[4mfloat32_42<10,16>\u001b[0m) -> float32_42<10,16>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mexp[torch]\u001b[0m(\u001b[4mfloat32_42<10,16>\u001b[0m) -> float32_45<10,16>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mneg[torch.Tensor]\u001b[0m(float32_45<10,16>\u001b[0m) -> float32_46<10,16>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mfloat[torch.Tensor]\u001b[0m(\u001b[4mfloat32_47<10>\u001b[0m) -> float32_47<10>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mLinear[torch.nn.modules.linear]\u001b[0m(float32_41<1,1,10>\u001b[0m) -> float32_49<1,1,33>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m └·\u001b[0m \u001b[0mlinear[torch.nn.functional]\u001b[0m(float32_41<1,1,10>\u001b[0m, float32_48<33,10>\u001b[0m, None) -> float32_49<1,1,33>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1msplit[torch.Tensor]\u001b[0m(float32_49<1,1,33>\u001b[0m, split_size=[1, int32_44<>\u001b[0m, int32_44<>\u001b[0m], dim=-1) -> (float32_50<1,1,1>\u001b[0m, float32_51<1,1,16>\u001b[0m, float32_52<1,1,16>\u001b[0m)\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mLinear[torch.nn.modules.linear]\u001b[0m(float32_50<1,1,1>\u001b[0m) -> float32_55<1,1,10>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m └·\u001b[0m \u001b[0mlinear[torch.nn.functional]\u001b[0m(float32_50<1,1,1>\u001b[0m, float32_53<10,1>\u001b[0m, float32_54<10>\u001b[0m) -> float32_55<1,1,10>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1msoftplus[torch.nn.functional]\u001b[0m(float32_55<1,1,10>\u001b[0m) -> float32_56<1,1,10>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mshape[nobuco]\u001b[0m(float32_41<1,1,10>\u001b[0m) -> (int32_57<>\u001b[0m, \u001b[90mint32_58<>\u001b[0m, int32_59<>\u001b[0m)\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mshape[nobuco]\u001b[0m(float32_46<10,16>\u001b[0m) -> (int32_60<>\u001b[0m, int32_61<>\u001b[0m)\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1meinsum[torch]\u001b[0m(\"abc,cd->abcd\", float32_56<1,1,10>\u001b[0m, float32_46<10,16>\u001b[0m) -> float32_62<1,1,10,16>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mexp[torch]\u001b[0m(float32_62<1,1,10,16>\u001b[0m) -> float32_63<1,1,10,16>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1meinsum[torch]\u001b[0m(\"abc,abd,abc->abcd\", float32_56<1,1,10>\u001b[0m, float32_51<1,1,16>\u001b[0m, float32_41<1,1,10>\u001b[0m) -> float32_64<1,1,10,16>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mzeros[torch]\u001b[0m((int32_57<>\u001b[0m, int32_59<>\u001b[0m, int32_61<>\u001b[0m), device=cpu) -> float32_65<1,10,16>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__getitem__[TensorBase]\u001b[0m(float32_63<1,1,10,16>\u001b[0m, (:, 0)) -> float32_66<1,10,16>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__mul__[torch.Tensor]\u001b[0m(float32_66<1,10,16>\u001b[0m, float32_65<1,10,16>\u001b[0m) -> float32_67<1,10,16>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__getitem__[TensorBase]\u001b[0m(float32_64<1,1,10,16>\u001b[0m, (:, 0)) -> float32_68<1,10,16>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__add__[torch.Tensor]\u001b[0m(float32_67<1,10,16>\u001b[0m, float32_68<1,10,16>\u001b[0m) -> float32_69<1,10,16>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__getitem__[TensorBase]\u001b[0m(float32_52<1,1,16>\u001b[0m, (:, 0, :)) -> float32_70<1,16>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1meinsum[torch]\u001b[0m(\"abc,ac->ab\", float32_69<1,10,16>\u001b[0m, float32_70<1,16>\u001b[0m) -> float32_71<1,10>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mstack[torch]\u001b[0m([float32_71<1,10>\u001b[0m], dim=1) -> float32_72<1,1,10>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__mul__[torch.Tensor]\u001b[0m(float32_41<1,1,10>\u001b[0m, \u001b[4mfloat32_47<10>\u001b[0m) -> float32_73<1,1,10>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__add__[torch.Tensor]\u001b[0m(float32_72<1,1,10>\u001b[0m, float32_73<1,1,10>\u001b[0m) -> float32_74<1,1,10>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1msilu[torch.nn.functional]\u001b[0m(float32_16<1,1,10>\u001b[0m) -> float32_75<1,1,10>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__mul__[torch.Tensor]\u001b[0m(float32_74<1,1,10>\u001b[0m, float32_75<1,1,10>\u001b[0m) -> float32_76<1,1,10>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m ├·\u001b[0m \u001b[32m\u001b[1mLinear[torch.nn.modules.linear]\u001b[0m(float32_76<1,1,10>\u001b[0m) -> float32_78<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m └ \u001b[0m \u001b[32m\u001b[1m └·\u001b[0m \u001b[0mlinear[torch.nn.functional]\u001b[0m(float32_76<1,1,10>\u001b[0m, float32_77<5,10>\u001b[0m, None) -> float32_78<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m └·\u001b[0m \u001b[32m\u001b[1m__add__[torch.Tensor]\u001b[0m(float32_78<1,1,5>\u001b[0m, float32_2<1,1,5>\u001b[0m) -> float32_79<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32mRMSNorm[model]\u001b[0m(float32_79<1,1,5>\u001b[0m) -> float32_86<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mpow[torch.Tensor]\u001b[0m(float32_79<1,1,5>\u001b[0m, 2) -> float32_80<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mmean[torch.Tensor]\u001b[0m(float32_80<1,1,5>\u001b[0m, -1, keepdim=True) -> float32_81<1,1,1>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__add__[torch.Tensor]\u001b[0m(float32_81<1,1,1>\u001b[0m, 1e-05) -> float32_82<1,1,1>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1mrsqrt[torch]\u001b[0m(float32_82<1,1,1>\u001b[0m) -> float32_83<1,1,1>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m │ \u001b[0m \u001b[32m\u001b[1m__mul__[torch.Tensor]\u001b[0m(float32_79<1,1,5>\u001b[0m, float32_83<1,1,1>\u001b[0m) -> float32_84<1,1,5>\u001b[0m\n",
      "\u001b[32m │ \u001b[0m \u001b[32m └·\u001b[0m \u001b[32m\u001b[1m__mul__[torch.Tensor]\u001b[0m(float32_84<1,1,5>\u001b[0m, \u001b[4mfloat32_85<5>\u001b[0m) -> float32_86<1,1,5>\u001b[0m\n",
      "\u001b[32m ├·\u001b[0m \u001b[32m\u001b[1mLinear[torch.nn.modules.linear]\u001b[0m(float32_86<1,1,5>\u001b[0m) -> float32_87<1,1,50280>\u001b[0m\n",
      "\u001b[32m └ \u001b[0m \u001b[32m\u001b[1m └·\u001b[0m \u001b[0mlinear[torch.nn.functional]\u001b[0m(float32_86<1,1,5>\u001b[0m, float32_1<50280,5>\u001b[0m, None) -> float32_87<1,1,50280>\u001b[0m\n",
      "\n",
      "Conversion complete. Elapsed time: 0.36 sec.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "args = ModelArgs(\n",
    "    d_model=5,\n",
    "    n_layer=1,\n",
    "    vocab_size=50277\n",
    ")\n",
    "model_1 = Mamba(args)\n",
    "model_1.eval()\n",
    "export_name = \"mamba_minimal_1_layer\"\n",
    "\n",
    "keras_model = nobuco.pytorch_to_keras(\n",
    "    model_1,\n",
    "    args=[input_ids], kwargs=None,\n",
    "    input_shapes={input_ids: (None, None)}, # Annotate dynamic axes with None\n",
    "    inputs_channel_order=ChannelOrder.TENSORFLOW,\n",
    "    outputs_channel_order=ChannelOrder.TENSORFLOW,\n",
    "    constants_to_variables=False,\n",
    "    trace_shape=True,\n",
    "    # save_trace_html=True\n",
    ")\n",
    "keras_model.save(f'{export_name}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c704940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# prompt\n",
    "dummy_prompt_keras = \"Harry\"\n",
    "input_ids_keras = tokenizer(dummy_prompt_keras, return_tensors='tf').input_ids  # Usa 'tf' per TensorFlow\n",
    "\n",
    "# loading model\n",
    "keras_model = tf.keras.models.load_model(f'{export_name}.h5')\n",
    "#keras_model.summary()\n",
    "\n",
    "# inference\n",
    "out = keras_model.predict(input_ids_keras)\n",
    "\n",
    "# output\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2efb17-37ad-472b-b029-9567acf17629",
   "metadata": {},
   "source": [
    "## (2) Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4b2d62d-0d95-4a3f-bd98-aa37e3f26b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             prompt: str,\n",
    "             n_tokens_to_gen: int = 50,\n",
    "             sample: bool = True,\n",
    "             top_k: int = 40):\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    \n",
    "    for token_n in range(n_tokens_to_gen):\n",
    "        with torch.no_grad():\n",
    "            indices_to_input = input_ids\n",
    "            next_token_logits = model(indices_to_input)[:, -1]\n",
    "        \n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        (batch, vocab_size) = probs.shape\n",
    "        \n",
    "        if top_k is not None:\n",
    "            (values, indices) = torch.topk(probs, k=top_k)\n",
    "            probs[probs < values[:, -1, None]] = 0\n",
    "            probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        if sample:\n",
    "            next_indices = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "    output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "    \n",
    "    return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee877143-2042-4579-8042-a96db6200517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba is the only one of the four dragons to be discovered by humans, so he knows that the world is the enemy and he tries to protect them. He is the only one to find out what the truth of the world is and therefore is sent to the dragon\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'Mamba is the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65d70549-597f-49ca-9185-2184d2576f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John: Hi!\n",
      "Sally: Hey!\n",
      "John: So, when's the wedding?\n",
      "Sally: We haven't decided.\n",
      "John: It's in September.\n",
      "Sally: Yeah, we were thinking July or\n",
      "August.\n",
      "John: I'm not too\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'John: Hi!\\nSally:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d419fc9-066b-4818-812c-2f1952528bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is \n",
      "just this: It is the best you can do.\n",
      "\n",
      "--K.J.\n",
      "\n",
      "And finally: How to handle your emotions. \n",
      "\n",
      "<|endoftext|>Q:\n",
      "\n",
      "Error creating an EntityManager instance in JavaEE 7\n",
      "\n",
      "This is\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'The meaning of life is '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b189e6e-6a96-4770-88cf-7c5de22cb321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter fan at me! I am sure it’ll go down very well!!\n",
      "\n",
      "It is so exciting and I have my doubts about one thing. I would love to see the film but I think it will be over before it begins.<|endoftext|>/*\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'Harry Potter'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
