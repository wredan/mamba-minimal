{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Keras Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow transformers nobuco\n",
    "\n",
    "# required install torch on https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nobuco\n",
    "from nobuco import ChannelOrder, ChannelOrderingStrategy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from model_pytorch import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nobuco.converter(F.softplus, channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS)\n",
    "def softplus(input: torch.Tensor):\n",
    "    return lambda input: tf.keras.activations.softplus(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nobuco.converter(torch.einsum, channel_ordering_strategy=ChannelOrderingStrategy.FORCE_PYTORCH_ORDER)\n",
    "def converter_einsum(*args):\n",
    "    def func(*args):\n",
    "        equation = args[0]\n",
    "        operands = args[1:]\n",
    "        print(\"----------------converter_einsum------------------\")\n",
    "        print(operands)\n",
    "        return tf.einsum(equation, *operands)\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ModelArgs(\n",
    "    d_model=5,\n",
    "    n_layer=1,\n",
    "    vocab_size=50277\n",
    ")\n",
    "model = Mamba(args)\n",
    "model.eval()\n",
    "export_name = \"mamba_minimal_1_layer\"\n",
    "dummy_input = \"Test\"\n",
    "input_ids = tokenizer(dummy_input, return_tensors='pt').input_ids\n",
    "\n",
    "keras_model = nobuco.pytorch_to_keras(\n",
    "    model,\n",
    "    args=[input_ids], kwargs=None,\n",
    "    input_shapes={input_ids: (1, None)}, # Annotate dynamic axes with None\n",
    "    inputs_channel_order=ChannelOrder.TENSORFLOW,\n",
    "    outputs_channel_order=ChannelOrder.TENSORFLOW,\n",
    "    constants_to_variables=False,\n",
    "    trace_shape=True,\n",
    "    save_trace_html=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(keras_model, f'{export_name}.keras')\n",
    "tf.keras.models.save_model(keras_model, f'{export_name}.h5')\n",
    "tf.saved_model.save(keras_model, f'{export_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Test for nobuco converted model\n",
    "# prompt\n",
    "dummy_prompt_keras = \"Harry Potter\"\n",
    "input_ids_keras = tokenizer(dummy_prompt_keras, return_tensors='tf').input_ids\n",
    "\n",
    "#input_ids_keras = tf.cast(input_ids_keras, tf.int64)\n",
    "# inference\n",
    "out = keras_model.predict(input_ids_keras)\n",
    "# output\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SavedModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "dummy_prompt_keras = \"Harry\"\n",
    "input_ids_keras = tokenizer(dummy_prompt_keras, return_tensors='tf').input_ids\n",
    "\n",
    "# loading model\n",
    "export_path = \"mamba_minimal_1_layer\" # No '.h5' in the path!\n",
    "keras_model_restored = tf.saved_model.load(export_path)\n",
    "\n",
    "input_ids_keras = tf.cast(input_ids_keras, tf.int64)\n",
    "\n",
    "# inference\n",
    "out = keras_model_restored(input_ids_keras)\n",
    "\n",
    "# output\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "dummy_prompt_keras = \"Harry Potter\"\n",
    "input_ids_keras = tokenizer(dummy_prompt_keras, return_tensors='tf').input_ids\n",
    "\n",
    "# loading model\n",
    "export_name = \"mamba_minimal_1_layer\"\n",
    "keras_model = tf.keras.saving.load_model (f'{export_name}.keras', safe_mode=False)\n",
    "\n",
    "input_ids_keras = tf.cast(input_ids_keras, tf.int64)\n",
    "\n",
    "# inference\n",
    "out = keras_model.predict(input_ids_keras)\n",
    "\n",
    "# output\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from nobuco.layers.weight import WeightLayer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "\n",
    "# prompt\n",
    "dummy_prompt_keras = \"Harry Potter\"\n",
    "input_ids_keras = tokenizer(dummy_prompt_keras, return_tensors='tf').input_ids\n",
    "\n",
    "# loading model\n",
    "export_name = \"mamba_minimal_1_layer\"\n",
    "\n",
    "custom_objects = {'WeightLayer': WeightLayer}\n",
    "\n",
    "keras_model = keras.saving.load_model (f'{export_name}.h5', custom_objects=custom_objects)\n",
    "\n",
    "input_ids_keras = tf.cast(input_ids_keras, tf.int64)\n",
    "\n",
    "# inference\n",
    "out = keras_model.predict(input_ids_keras)\n",
    "\n",
    "# output\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx2keras import onnx_to_keras\n",
    "\n",
    "# Load ONNX model\n",
    "onnx_model = onnx.load('mamba_minimal_1_layer.onnx')\n",
    "\n",
    "# Call the converter (input - is the main model input name, can be different for your model)\n",
    "k_model = onnx_to_keras(onnx_model, ['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load('mamba_minimal_1_layer.onnx')\n",
    "\n",
    "tf_rep = prepare(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea il convertitore per il modello salvato\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"mamba_minimal_1_layer\")\n",
    "\n",
    "# Converti il modello\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Salva il modello TFLite\n",
    "with open(\"tflite_mamba_minimal_1_layer.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=\"tflite_mamba_minimal_1_layer.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# prompt\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "dummy_prompt_keras = \"Harry tt\"\n",
    "input_ids_keras = tokenizer(dummy_prompt_keras, return_tensors='tf').input_ids\n",
    "input_ids_keras_int64 = tf.cast(input_ids_keras, tf.int64)\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], input_ids_keras_int64)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
