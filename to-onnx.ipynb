{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilosantitto/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model_pytorch import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# One of:\n",
    "#     'state-spaces/mamba-2.8b-slimpj'\n",
    "#     'state-spaces/mamba-2.8b'\n",
    "#     'state-spaces/mamba-1.4b'\n",
    "#     'state-spaces/mamba-790m'\n",
    "#     'state-spaces/mamba-370m'\n",
    "#     'state-spaces/mamba-130m'\n",
    "pretrained_model_name = 'state-spaces/mamba-130m'\n",
    "dummy_input = \"test\"\n",
    "\n",
    "model = Mamba.from_pretrained(pretrained_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "input_ids = tokenizer(dummy_input, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilosantitto/miniconda3/envs/deepl/lib/python3.11/site-packages/nobuco/trace/trace.py:339: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  outputs = orig_method(*args, **kwargs)\n",
      "/Users/danilosantitto/miniconda3/envs/deepl/lib/python3.11/site-packages/nobuco/trace/trace.py:339: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  outputs = orig_method(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Export model\n",
    "export_name = \"mamba_model\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    input_ids,  \n",
    "    f\"{export_name}.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'seq_length'},  \n",
    "        'output': {0: 'batch_size', 1: 'seq_length'}\n",
    "    }\n",
    ")\n",
    "torch.save(model, f\"{export_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilosantitto/miniconda3/envs/deepl/lib/python3.11/site-packages/nobuco/trace/trace.py:339: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  outputs = orig_method(*args, **kwargs)\n",
      "/Users/danilosantitto/miniconda3/envs/deepl/lib/python3.11/site-packages/nobuco/trace/trace.py:339: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  outputs = orig_method(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Export minimal model\n",
    "\n",
    "args = ModelArgs(\n",
    "    d_model=5,\n",
    "    n_layer=1,\n",
    "    vocab_size=50277\n",
    ")\n",
    "model_1 = Mamba(args)\n",
    "model_1.eval()\n",
    "export_name = \"mamba_minimal_1_layer\"\n",
    "\n",
    "torch.save(model_1, f\"{export_name}.pt\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_1,\n",
    "    input_ids,  \n",
    "    f\"{export_name}.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'seq_length'},  \n",
    "        'output': {0: 'batch_size', 1: 'seq_length'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(1, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[[47.18734  , 37.300926 , 49.27418  , ..., 37.411118 ,\n",
       "          37.17351  , 37.505386 ],\n",
       "         [17.432875 ,  6.7534537, 17.546019 , ...,  6.6828403,\n",
       "           6.663456 ,  6.74733  ],\n",
       "         [-5.2570963, -8.190227 , -5.680652 , ..., -8.190908 ,\n",
       "          -8.089128 , -8.0610895],\n",
       "         [50.62617  , 37.7902   , 52.534973 , ..., 37.788456 ,\n",
       "          37.55665  , 37.887684 ]]], dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_session = ort.InferenceSession('mamba_model.onnx')\n",
    "\n",
    "# Generate a model dummy input\n",
    "dummy_prompt_1 = \"Harry test ciao\"  \n",
    "tokens_1 = tokenizer(dummy_prompt_1, return_tensors=\"pt\")\n",
    "input_ids_1 = tokens_1.input_ids.to(device=\"cpu\")\n",
    "input_ids_np = np.array(input_ids_1)\n",
    "print(input_ids_np.shape)\n",
    "\n",
    "# Inference\n",
    "inputs = {ort_session.get_inputs()[0].name: input_ids_np}\n",
    "out = ort_session.run(None, inputs)\n",
    "\n",
    "# Output\n",
    "print(input_ids_np.shape)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "ort_session = ort.InferenceSession('mamba_model.onnx')\n",
    "\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             prompt: str,\n",
    "             n_tokens_to_gen: int = 10,\n",
    "             sample: bool = True,\n",
    "             top_k: int = 40):\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "        \n",
    "    for token_n in range(n_tokens_to_gen):\n",
    "        with torch.no_grad():\n",
    "            indices_to_input = input_ids\n",
    "            inputs = {ort_session.get_inputs()[0].name: np.array(indices_to_input)}\n",
    "            # Utilizzare np.squeeze per rimuovere le dimensioni singole iniziali o specifiche\n",
    "            output_array = np.array(ort_session.run(None, inputs))\n",
    "            output_tensor = torch.from_numpy(output_array).squeeze(0)  # Rimuove la dimensione extra in posizione 0\n",
    "            next_token_logits = output_tensor[:, -1, :]  # Seleziona l'ultimo token generato\n",
    "            # La dimensione ora sar√† [1, 50280], corrispondente all'ultimo set di logits\n",
    "            print(next_token_logits.shape)\n",
    "\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        (batch, vocab_size) = probs.shape\n",
    "        \n",
    "        if top_k is not None:\n",
    "            (values, indices) = torch.topk(probs, k=top_k)\n",
    "            probs[probs < values[:, -1, None]] = 0\n",
    "            probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        if sample:\n",
    "            next_indices = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "    output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "    \n",
    "    return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50280])\n",
      "torch.Size([1, 50280])\n",
      "torch.Size([1, 50280])\n",
      "torch.Size([1, 50280])\n",
      "torch.Size([1, 50280])\n",
      "torch.Size([1, 50280])\n",
      "torch.Size([1, 50280])\n",
      "torch.Size([1, 50280])\n",
      "torch.Size([1, 50280])\n",
      "torch.Size([1, 50280])\n",
      "Harry Potter is  LINEAR AND DO.\n",
      "\n",
      "    disappe disappe disappe disappe\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, prompt=\"Harry Potter is \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dynamic model (Beta)\n",
    "\n",
    "args = ModelArgs(\n",
    "    d_model=5,\n",
    "    n_layer=1,\n",
    "    vocab_size=50277\n",
    ")\n",
    "model_dyn = Mamba(args)\n",
    "model_dyn.eval()\n",
    "export_name = \"mamba_minimal_1_layer_dyn\"\n",
    "\n",
    "torch.onnx.dynamo_export(\n",
    "    model_dyn,\n",
    "    input_ids,\n",
    "    export_options=torch.onnx.ExportOptions(dynamic_shapes=True)\n",
    ").save(f\"{export_name}.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
