{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_pytorch import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# One of:\n",
    "#     'state-spaces/mamba-2.8b-slimpj'\n",
    "#     'state-spaces/mamba-2.8b'\n",
    "#     'state-spaces/mamba-1.4b'\n",
    "#     'state-spaces/mamba-790m'\n",
    "#     'state-spaces/mamba-370m'\n",
    "#     'state-spaces/mamba-130m'\n",
    "pretrained_model_name = 'state-spaces/mamba-130m'\n",
    "dummy_input = \"Harry Potter\"\n",
    "\n",
    "model = Mamba.from_pretrained(pretrained_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "input_ids = tokenizer(dummy_input, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export model\n",
    "export_name = \"mamba_model_130m_cumsum_no_einsum\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    input_ids,  \n",
    "    f\"{export_name}.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'seq_length'},  \n",
    "        'output': {0: 'batch_size', 1: 'seq_length'}\n",
    "    }\n",
    ")\n",
    "torch.save(model, f\"{export_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export minimal model\n",
    "\n",
    "args = ModelArgs(\n",
    "    d_model=5,\n",
    "    n_layer=1,\n",
    "    vocab_size=50277\n",
    ")\n",
    "model = Mamba(args)\n",
    "model.eval()\n",
    "export_name = \"mamba_minimal_1_layer_cumsum_no_einsum\"\n",
    "\n",
    "torch.save(model, f\"{export_name}.pt\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    input_ids,  \n",
    "    f\"{export_name}.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'seq_length'},  \n",
    "        'output': {0: 'batch_size', 1: 'seq_length'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"mamba_model_130m_cumsum_no_einsum.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "#print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[[ -8.417368 , -22.110765 ,  -2.4200068, ..., -21.946516 ,\n",
       "          -21.980217 , -21.913406 ],\n",
       "         [  0.3511305, -26.0508   ,   1.545126 , ..., -25.823263 ,\n",
       "          -26.040113 , -25.818874 ],\n",
       "         [-40.83871  , -53.83088  , -38.9781   , ..., -53.959682 ,\n",
       "          -53.70117  , -53.945526 ]]], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_session = ort.InferenceSession('mamba_model_130m_cumsum_no_einsum.onnx')\n",
    "\n",
    "# Generate a model dummy input\n",
    "dummy_prompt_1 = \"Harry Potter test\"  \n",
    "tokens_1 = tokenizer(dummy_prompt_1, return_tensors=\"pt\")\n",
    "input_ids_1 = tokens_1.input_ids.to(device=\"cpu\")\n",
    "input_ids_np = np.array(input_ids_1)\n",
    "print(input_ids_np.shape)\n",
    "\n",
    "# Inference\n",
    "inputs = {ort_session.get_inputs()[0].name: input_ids_np}\n",
    "onnx_out = ort_session.run(None, inputs)\n",
    "\n",
    "# Output\n",
    "onnx_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -8.4174, -22.1108,  -2.4200,  ..., -21.9465, -21.9803, -21.9134],\n",
      "         [  0.3512, -26.0508,   1.5452,  ..., -25.8232, -26.0401, -25.8188],\n",
      "         [-40.8387, -53.8309, -38.9781,  ..., -53.9597, -53.7011, -53.9456]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "torch_out = model(input_ids_1)\n",
    "print(torch_out)\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "try:\n",
    "    np.testing.assert_allclose(torch_out.detach().cpu().numpy(), onnx_out[0], rtol=1e-02, atol=1e-03)\n",
    "    print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")\n",
    "except AssertionError as e:\n",
    "    print(\"AssertionError:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             prompt: str,\n",
    "             n_tokens_to_gen: int = 50,\n",
    "             sample: bool = True,\n",
    "             top_k: int = 40):\n",
    "    \n",
    "    ort_session = ort.InferenceSession(model)\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "        \n",
    "    for _ in range(n_tokens_to_gen):\n",
    "        with torch.no_grad():\n",
    "            indices_to_input = input_ids\n",
    "            \n",
    "            inputs = {ort_session.get_inputs()[0].name: np.array(indices_to_input)}\n",
    "            output_array = np.array(ort_session.run(None, inputs))\n",
    "            output_tensor = torch.from_numpy(output_array).squeeze(0)  # Rimuove la dimensione extra in posizione 0\n",
    "            next_token_logits = output_tensor[:, -1]  # Seleziona l'ultimo token generato\n",
    "\n",
    "\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        (batch, vocab_size) = probs.shape\n",
    "        \n",
    "        if top_k is not None:\n",
    "            (values, indices) = torch.topk(probs, k=top_k)\n",
    "            probs[probs < values[:, -1, None]] = 0\n",
    "            probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        if sample:\n",
    "            next_indices = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "    output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "    \n",
    "    return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter is not only written with a happy and happy mind. Its intent – to make us all feel better than\n"
     ]
    }
   ],
   "source": [
    "print(generate(model='mamba_model_130m_cumsum_no_einsum.onnx', tokenizer=tokenizer, prompt=\"Harry Potter is\", n_tokens_to_gen=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([4, 5])\n",
      "torch.Size([2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# Definiamo la dimensione di ogni asse\n",
    "b, l, d, n = 2, 3, 4, 5\n",
    "\n",
    "# Creiamo tensori di esempio con le dimensioni appropriate\n",
    "dt = torch.randn(b, l, d)  # Tensore con forma (b, l, d)\n",
    "A = torch.randn(d, n)     # Tensore con forma (d, n)\n",
    "\n",
    "print(dt.shape)\n",
    "print(A.shape)\n",
    "# Utilizziamo torch.einsum per moltiplicare dt e A secondo la regola specificata\n",
    "result = torch.einsum('bld,dn->bldn', dt, A)\n",
    "\n",
    "# Mostreremo la forma del tensore di risultato per confermare che è (b, l, d, n)\n",
    "print(result.shape)  # Dovrebbe stampare: torch.Size([b, l, d, n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Dimensioni di esempio\n",
    "b, l, d, n = 2, 3, 4, 5\n",
    "\n",
    "# Tensori di esempio\n",
    "dt = torch.randn(b, l, d)\n",
    "A = torch.randn(d, n)\n",
    "\n",
    "# Operazione senza einsum\n",
    "# Passo 1: Ridimensiona A per il broadcasting\n",
    "A_expanded = A.view(1, 1, d, n).expand(b, l, d, n)\n",
    "\n",
    "# Passo 2: Moltiplica dt per A_expanded\n",
    "# Poiché vogliamo mantenere dt inalterato e solo \"applicare\" A a ogni elemento, \n",
    "# dobbiamo prima aggiungere dimensioni a dt per il broadcasting.\n",
    "dt_expanded = dt.unsqueeze(-1)  # Aggiunge una dimensione alla fine per il broadcasting\n",
    "\n",
    "# Moltiplicazione elemento per elemento\n",
    "result = dt_expanded * A_expanded\n",
    "\n",
    "# Verifica la forma del risultato\n",
    "print(result.shape)  # Dovrebbe essere torch.Size([b, l, d, n])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Dimensioni di esempio\n",
    "b, l, d, n = 2, 3, 4, 5\n",
    "\n",
    "# Tensori di esempio\n",
    "dt = torch.randn(b, l, d)\n",
    "u = torch.randn(b, l, d)\n",
    "B = torch.randn(b, l, n)\n",
    "\n",
    "# Passo 1: Moltiplicazione elemento per elemento di dt e u\n",
    "dt_u_product = dt * u  # Il risultato ha forma (b, l, d)\n",
    "\n",
    "# Passo 2: Espandi il risultato aggiungendo una nuova dimensione per il broadcasting\n",
    "dt_u_expanded = dt_u_product.unsqueeze(-1)  # Aggiunge una dimensione fittizia alla fine, forma (b, l, d, 1)\n",
    "\n",
    "# Passo 3: Espandi B per il broadcasting\n",
    "B_expanded = B.unsqueeze(2)  # Aggiunge una dimensione fittizia nella terza posizione, forma (b, l, 1, n)\n",
    "\n",
    "# Passo 4: Moltiplicazione elemento per elemento con broadcasting\n",
    "deltaB_u = dt_u_expanded * B_expanded  # Il risultato ha forma (b, l, d, n)\n",
    "\n",
    "# Controlla la forma del risultato\n",
    "print(deltaB_u.shape)  # Dovrebbe essere torch.Size([b, l, d, n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Dimensioni di esempio\n",
    "b, l, d, n = 2, 3, 4, 5\n",
    "\n",
    "# Tensori di esempio\n",
    "x = torch.randn(b, l, d, n)\n",
    "C = torch.randn(b, l, n)\n",
    "\n",
    "# Espandi C per il broadcasting\n",
    "C_expanded = C.unsqueeze(2)  # La forma diventa (b, l, 1, n)\n",
    "\n",
    "# Moltiplica x e C con broadcasting\n",
    "product = torch.mul(x, C_expanded)\n",
    "\n",
    "# Somma lungo l'asse n per ottenere la forma finale (b, l, d)\n",
    "y = torch.sum(product, dim=-1)\n",
    "\n",
    "# Controlla la forma del risultato\n",
    "print(y.shape)  # Dovrebbe essere torch.Size([b, l, d])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             prompt: str,\n",
    "             n_tokens_to_gen: int = 50,\n",
    "             sample: bool = True,\n",
    "             top_k: int = 40):\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    \n",
    "    for _ in range(n_tokens_to_gen):\n",
    "        with torch.no_grad():\n",
    "            indices_to_input = input_ids\n",
    "            next_token_logits = model(indices_to_input)[:, -1]\n",
    "        \n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        (batch, vocab_size) = probs.shape\n",
    "        \n",
    "        if top_k is not None:\n",
    "            (values, indices) = torch.topk(probs, k=top_k)\n",
    "            probs[probs < values[:, -1, None]] = 0\n",
    "            probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        if sample:\n",
    "            next_indices = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "    output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "    \n",
    "    return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter is a fantastic tool in making your day-to-day life a lot more interesting – and not the\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, prompt=\"Harry Potter is\", n_tokens_to_gen=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dynamic model (Beta)\n",
    "\n",
    "args = ModelArgs(\n",
    "    d_model=5,\n",
    "    n_layer=1,\n",
    "    vocab_size=50277\n",
    ")\n",
    "model_dyn = Mamba(args)\n",
    "model_dyn.eval()\n",
    "export_name = \"mamba_minimal_1_layer_dyn\"\n",
    "\n",
    "torch.onnx.dynamo_export(\n",
    "    model_dyn,\n",
    "    input_ids,\n",
    "    export_options=torch.onnx.ExportOptions(dynamic_shapes=True)\n",
    ").save(f\"{export_name}.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
