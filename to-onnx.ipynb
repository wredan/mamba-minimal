{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_pytorch import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import onnx\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of:\n",
    "#     'state-spaces/mamba-2.8b-slimpj'\n",
    "#     'state-spaces/mamba-2.8b'\n",
    "#     'state-spaces/mamba-1.4b'\n",
    "#     'state-spaces/mamba-790m'\n",
    "#     'state-spaces/mamba-370m'\n",
    "#     'state-spaces/mamba-130m'\n",
    "pretrained_model_name = 'state-spaces/mamba-130m'\n",
    "dummy_input = \"Harry Potter\"\n",
    "\n",
    "model = Mamba.from_pretrained(pretrained_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
    "input_ids = tokenizer(dummy_input, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export model\n",
    "export_name = \"mamba_model_130m_cumsum_no_einsum\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    input_ids,  \n",
    "    f\"{export_name}.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'seq_length'},  \n",
    "        'output': {0: 'batch_size', 1: 'seq_length'}\n",
    "    }\n",
    ")\n",
    "torch.save(model, f\"{export_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export minimal model\n",
    "\n",
    "args = ModelArgs(\n",
    "    d_model=5,\n",
    "    n_layer=1,\n",
    "    vocab_size=50277\n",
    ")\n",
    "model = Mamba(args)\n",
    "model.eval()\n",
    "export_name = \"mamba_minimal_1_layer_cumsum_no_einsum\"\n",
    "\n",
    "torch.save(model, f\"{export_name}.pt\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    input_ids,  \n",
    "    f\"{export_name}.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'seq_length'},  \n",
    "        'output': {0: 'batch_size', 1: 'seq_length'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "model_onnx = onnx.load(\"mamba_model_130m_cumsum_no_einsum.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model_onnx)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "#print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[[ -8.417368 , -22.110765 ,  -2.4200068, ..., -21.946516 ,\n",
       "          -21.980217 , -21.913406 ],\n",
       "         [  0.3511305, -26.0508   ,   1.545126 , ..., -25.823263 ,\n",
       "          -26.040113 , -25.818874 ],\n",
       "         [-40.83871  , -53.83088  , -38.9781   , ..., -53.959682 ,\n",
       "          -53.70117  , -53.945526 ]]], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_session = ort.InferenceSession('mamba_model_130m_cumsum_no_einsum.onnx')\n",
    "\n",
    "# Generate a model dummy input\n",
    "dummy_prompt_1 = \"Harry Potter test\"  \n",
    "tokens_1 = tokenizer(dummy_prompt_1, return_tensors=\"pt\")\n",
    "input_ids_1 = tokens_1.input_ids.to(device=\"cpu\")\n",
    "input_ids_np = np.array(input_ids_1)\n",
    "print(input_ids_np.shape)\n",
    "\n",
    "# Inference\n",
    "inputs = {ort_session.get_inputs()[0].name: input_ids_np}\n",
    "onnx_out = ort_session.run(None, inputs)\n",
    "\n",
    "# Output\n",
    "onnx_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing PyTorch and ONNX inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -8.4174, -22.1108,  -2.4200,  ..., -21.9465, -21.9803, -21.9134],\n",
      "         [  0.3512, -26.0508,   1.5452,  ..., -25.8232, -26.0401, -25.8188],\n",
      "         [-40.8387, -53.8309, -38.9781,  ..., -53.9597, -53.7011, -53.9456]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "torch_out = model(input_ids_1)\n",
    "print(torch_out)\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "try:\n",
    "    np.testing.assert_allclose(torch_out.detach().cpu().numpy(), onnx_out[0], rtol=1e-02, atol=1e-03)\n",
    "    print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")\n",
    "except AssertionError as e:\n",
    "    print(\"AssertionError:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token_logits(model, indices_to_input, is_onnx=False):\n",
    "    if is_onnx:\n",
    "        inputs = {model.get_inputs()[0].name: np.array(indices_to_input)}\n",
    "        output_array = np.array(model.run(None, inputs))\n",
    "        output_tensor = torch.from_numpy(output_array).squeeze(0)  # Remove numpy extra dim\n",
    "        return output_tensor[:, -1]  # Select last generated token\n",
    "    else:\n",
    "        return model(indices_to_input)[:, -1]\n",
    "\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             prompt: str,\n",
    "             n_tokens_to_gen: int = 50,\n",
    "             sample: bool = True,\n",
    "             top_k: int = 40,\n",
    "             is_onnx=False):\n",
    "    \n",
    "    if is_onnx:\n",
    "        model = ort.InferenceSession(model)\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "        \n",
    "    for _ in range(n_tokens_to_gen):\n",
    "        with torch.no_grad():\n",
    "            indices_to_input = input_ids\n",
    "            next_token_logits = get_next_token_logits(model, indices_to_input, is_onnx)\n",
    "\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        (batch, vocab_size) = probs.shape\n",
    "        \n",
    "        if top_k is not None:\n",
    "            (values, indices) = torch.topk(probs, k=top_k)\n",
    "            probs[probs < values[:, -1, None]] = 0\n",
    "            probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        if sample:\n",
    "            next_indices = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "    output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "    \n",
    "    return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter is not your child. You have the ability to change it for the best of you, to make your son a better person in life. You may even be able to change him for you. But you can only change who you always make it. You\n"
     ]
    }
   ],
   "source": [
    "print(generate(model='mamba_model_130m_cumsum_no_einsum.onnx', is_onnx=True, tokenizer=tokenizer, prompt=\"Harry Potter is\", n_tokens_to_gen=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter is over, then I'm really glad you're not going to my parents! That is, if you were to have a mother and a father, that would be the best. The man who comes on like an ice-fish has got enough heart disease\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, prompt=\"Harry Potter is\", n_tokens_to_gen=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dynamic export (Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dynamic model (Beta)\n",
    "\n",
    "args = ModelArgs(\n",
    "    d_model=5,\n",
    "    n_layer=1,\n",
    "    vocab_size=50277\n",
    ")\n",
    "model_dyn = Mamba(args)\n",
    "model_dyn.eval()\n",
    "export_name = \"mamba_minimal_1_layer_dyn\"\n",
    "\n",
    "torch.onnx.dynamo_export(\n",
    "    model_dyn,\n",
    "    input_ids,\n",
    "    export_options=torch.onnx.ExportOptions(dynamic_shapes=True)\n",
    ").save(f\"{export_name}.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of how to implements einsum alternative operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([4, 5])\n",
      "torch.Size([2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensions of each axis\n",
    "b, l, d, n = 2, 3, 4, 5\n",
    "\n",
    "# Create example tensors with appropriate dimensions\n",
    "dt = torch.randn(b, l, d)  # Tensor with shape (b, l, d)\n",
    "A = torch.randn(d, n)      # Tensor with shape (d, n)\n",
    "\n",
    "print(dt.shape)\n",
    "print(A.shape)\n",
    "# Use torch.einsum to multiply dt and A according to the specified rule\n",
    "result = torch.einsum('bld,dn->bldn', dt, A)\n",
    "\n",
    "# Display the shape of the result tensor to confirm it is (b, l, d, n)\n",
    "print(result.shape)  # Should print: torch.Size([b, l, d, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# EXPANSION OF deltaA = torch.einsum('bld,dn->bldn', dt, A)\n",
    "\n",
    "# Example dimensions\n",
    "b, l, d, n = 2, 3, 4, 5\n",
    "\n",
    "# Example tensors\n",
    "dt = torch.randn(b, l, d)\n",
    "A = torch.randn(d, n)\n",
    "\n",
    "# Operation without einsum\n",
    "# Step 1: Resize A for broadcasting\n",
    "A_expanded = A.view(1, 1, d, n).expand(b, l, d, n)\n",
    "\n",
    "# Step 2: Multiply dt by A_expanded\n",
    "# Since we want to keep dt unchanged and only \"apply\" A to each element,\n",
    "# we need to first add a dimension to dt for broadcasting.\n",
    "dt_expanded = dt.unsqueeze(-1)  # Adds a dimension at the end for broadcasting\n",
    "\n",
    "# Element-wise multiplication\n",
    "result = dt_expanded * A_expanded\n",
    "\n",
    "# Verify the shape of the result\n",
    "print(result.shape)  # Should be torch.Size([b, l, d, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# EXPANSION OF deltaB_u = torch.einsum('bld,bld,bln->bldn', dt, u, B)\n",
    "\n",
    "# Example dimensions\n",
    "b, l, d, n = 2, 3, 4, 5\n",
    "\n",
    "# Example tensors\n",
    "dt = torch.randn(b, l, d)\n",
    "u = torch.randn(b, l, d)\n",
    "B = torch.randn(b, l, n)\n",
    "\n",
    "# Step 1: Element-wise multiplication of dt and u\n",
    "dt_u_product = dt * u  # The result has shape (b, l, d)\n",
    "\n",
    "# Step 2: Expand the result by adding a new dimension for broadcasting\n",
    "dt_u_expanded = dt_u_product.unsqueeze(-1)  # Adds a dummy dimension at the end, shape (b, l, d, 1)\n",
    "\n",
    "# Step 3: Expand B for broadcasting\n",
    "B_expanded = B.unsqueeze(2)  # Adds a dummy dimension in the third position, shape (b, l, 1, n)\n",
    "\n",
    "# Step 4: Element-wise multiplication with broadcasting\n",
    "deltaB_u = dt_u_expanded * B_expanded  # The result has shape (b, l, d, n)\n",
    "\n",
    "# Check the shape of the result\n",
    "print(deltaB_u.shape)  # Should be torch.Size([b, l, d, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# EXPANSION OF y = torch.einsum('bldn,bln->bld', x, C)\n",
    "\n",
    "# Example dimensions\n",
    "b, l, d, n = 2, 3, 4, 5\n",
    "\n",
    "# Example tensors\n",
    "x = torch.randn(b, l, d, n)\n",
    "C = torch.randn(b, l, n)\n",
    "\n",
    "# Expand C for broadcasting\n",
    "C_expanded = C.unsqueeze(2)  # Shape becomes (b, l, 1, n)\n",
    "\n",
    "# Multiply x and C with broadcasting\n",
    "product = torch.mul(x, C_expanded)\n",
    "\n",
    "# Sum along the n axis to get the final shape (b, l, d)\n",
    "y = torch.sum(product, dim=-1)\n",
    "\n",
    "# Check the shape of the result\n",
    "print(y.shape)  # Should be torch.Size([b, l, d])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
